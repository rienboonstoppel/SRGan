{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from generator import *\n",
    "from discriminator import *\n",
    "from feature_extractor import *\n",
    "from dataset import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "import itertools\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num = 100206\n",
    "img = ImagePair(number=num, root_dir='data')\n",
    "slice = img.img()['LR'][:,:,25]\n",
    "plt.imshow(slice, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "tra_set = ImagePairDataset('training', transform=transform)\n",
    "val_set = ImagePairDataset('validation', transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Length of training set: \\t{}\\nLength of validation set: \\t{}'\n",
    "      .format(len(tra_set),len(val_set)))\n",
    "\n",
    "num = 25\n",
    "sample = tra_set[num]\n",
    "title = 'Image pair {}'.format(sample['id'])\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "fig.set_facecolor('white')\n",
    "fig.suptitle(title)\n",
    "ax1.imshow(np.squeeze(sample['LR']),cmap='gray')\n",
    "ax1.set_title('LR')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(np.squeeze(sample['HR']),cmap ='gray')\n",
    "ax2.set_title('HR')\n",
    "ax2.axis('off')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_cpu = 2\n",
    "tra_dataloader = DataLoader(\n",
    "    tra_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = GeneratorRRDB(channels=1, filters=64, num_res_blocks=1).cuda()\n",
    "summary(generator, (1, 224, 224))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]             640\n",
      "         LeakyReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 112, 112]          36,928\n",
      "       BatchNorm2d-4         [-1, 64, 112, 112]             128\n",
      "         LeakyReLU-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "       BatchNorm2d-7        [-1, 128, 112, 112]             256\n",
      "         LeakyReLU-8        [-1, 128, 112, 112]               0\n",
      "            Conv2d-9          [-1, 128, 56, 56]         147,584\n",
      "      BatchNorm2d-10          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-11          [-1, 128, 56, 56]               0\n",
      "           Conv2d-12          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-13          [-1, 256, 56, 56]             512\n",
      "        LeakyReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 28, 28]         590,080\n",
      "      BatchNorm2d-16          [-1, 256, 28, 28]             512\n",
      "        LeakyReLU-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-19          [-1, 512, 28, 28]           1,024\n",
      "        LeakyReLU-20          [-1, 512, 28, 28]               0\n",
      "           Conv2d-21          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-22          [-1, 512, 14, 14]           1,024\n",
      "        LeakyReLU-23          [-1, 512, 14, 14]               0\n",
      "           Conv2d-24            [-1, 1, 14, 14]           4,609\n",
      "================================================================\n",
      "Total params: 4,692,545\n",
      "Trainable params: 4,692,545\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 147.77\n",
      "Params size (MB): 17.90\n",
      "Estimated Total Size (MB): 165.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(input_shape=(1,224,224)).cuda()\n",
    "summary(discriminator, (1, 224, 224))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/model_vis')\n",
    "writer.add_graph(generator, sample['LR'].cuda())\n",
    "writer.add_graph(discriminator, sample['LR'].cuda())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "generator = GeneratorRRDB(channels=1, filters=64, num_res_blocks=1).to(device)\n",
    "discriminator = Discriminator(input_shape=(1,224,224)).to(device)\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "\n",
    "# Set feature extractor to inference mode\n",
    "feature_extractor.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 generator,\n",
    "                 discriminator,\n",
    "                 training_loader,\n",
    "                 validation_loader,\n",
    "                 feature_extractor,\n",
    "                 lr = 0.0002,\n",
    "                 b1 = 0.9,\n",
    "                 b2 = 0.999,\n",
    "                 epochs = 10,\n",
    "                 warmup_batches = 100,\n",
    "                 lambda_adv = 5e-3,\n",
    "                 lambda_pixel = 1e-2,\n",
    "                 sample_interval = 100,\n",
    "                 checkpoint_interval = 1000,\n",
    "                 ):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "        self.netG = generator.to(self.device)\n",
    "        self.netD = discriminator.to(self.device)\n",
    "        self.netF = feature_extractor.to(self.device)\n",
    "\n",
    "        self.criterion_GAN = torch.nn.BCEWithLogitsLoss().to(self.device)\n",
    "        self.criterion_content = torch.nn.L1Loss().to(self.device)\n",
    "        self.criterion_pixel = torch.nn.L1Loss().to(self.device)\n",
    "\n",
    "        self.lambda_adv = lambda_adv,\n",
    "        self.lambda_pixel = lambda_pixel,\n",
    "        self.sample_interval = sample_interval\n",
    "\n",
    "        self.training_loader = training_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=lr, betas=(b1, b2))\n",
    "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=lr, betas=(b1, b2))\n",
    "        self.metric = {\n",
    "            'train_loss_G': [],\n",
    "            'train_loss_content_G': [],\n",
    "            'train_loss_adversarial_G': [],\n",
    "            'train_loss_pixel_G': [],\n",
    "            'train_loss_D': [],\n",
    "            'val_loss_G': [],\n",
    "            'val_loss_content_G': [],\n",
    "            'val_loss_adversarial_G': [],\n",
    "            'val_loss_pixel_G': [],\n",
    "            'val_loss_D': [],\n",
    "\n",
    "        }\n",
    "        # self.output_dir = output_dir\n",
    "        # os.makedirs(output_dir, exist_ok=True)\n",
    "        # self.writer = SummaryWriter(output_dir)\n",
    "\n",
    "    def training(self, train_batch):\n",
    "        self.netG.train()\n",
    "        self.netD.train()\n",
    "        imgs_lr = Variable(train_batch['LR'].type(self.Tensor))\n",
    "        imgs_hr = Variable(train_batch['HR'].type(self.Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(self.Tensor(np.ones((imgs_lr.size(0), *self.netD.output_shape))), requires_grad=False)\n",
    "        fake = Variable(self.Tensor(np.zeros((imgs_lr.size(0), *self.netD.output_shape))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        self.optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a high resolution image from low resolution input\n",
    "        gen_hr = self.netG(imgs_lr)\n",
    "\n",
    "        # Measure pixel-wise loss against ground truth\n",
    "        loss_pixel = self.criterion_pixel(gen_hr, imgs_hr)\n",
    "\n",
    "        # if batches_done < warmup_batches:\n",
    "        #     # Warm-up (pixel-wise loss only)\n",
    "        #     loss_pixel.backward()\n",
    "        #     optimizer_G.step()\n",
    "        #     print(\n",
    "        #         \"[Epoch %d/%d] [Batch %d/%d] [G pixel: %f]\"\n",
    "        #         % (epoch, epochs, i, len(tra_dataloader), loss_pixel.item())\n",
    "        #     )\n",
    "        #     continue\n",
    "\n",
    "        # Extract validity predictions from discriminator\n",
    "        pred_real = self.netD(imgs_hr).detach()\n",
    "        pred_fake = self.netD(gen_hr)\n",
    "\n",
    "        # Adversarial loss (relativistic average GAN)\n",
    "        loss_GAN = self.criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "        # Content loss\n",
    "        gen_features = self.netF(torch.repeat_interleave(gen_hr,3,1))\n",
    "        real_features = self.netF(torch.repeat_interleave(imgs_hr,3,1)).detach()\n",
    "        loss_content = self.criterion_content(gen_features, real_features)\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_G = loss_content + self.lambda_adv * loss_GAN + self.lambda_pixel * loss_pixel\n",
    "\n",
    "        loss_G.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        self.optimizer_D.zero_grad()\n",
    "\n",
    "        pred_real = self.netD(imgs_hr)\n",
    "        pred_fake = self.netD(gen_hr.detach())\n",
    "\n",
    "        # Adversarial loss for real and fake images (relativistic average GAN)\n",
    "        loss_real = self.criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "        loss_fake = self.criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D.backward()\n",
    "        self.optimizer_D.step()\n",
    "\n",
    "        self.metric['train_loss_G'].append(loss_G.item())\n",
    "        self.metric['train_loss_content_G'].append(loss_content.item())\n",
    "        self.metric['train_loss_adversarial_G'].append(loss_GAN.item())\n",
    "        self.metric['train_loss_pixel_G'].append(loss_pixel.item())\n",
    "        self.metric['train_loss_D'].append(loss_D.item())\n",
    "        return gen_hr\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, validation_batch):\n",
    "        self.netG.eval()\n",
    "        self.netD.eval()\n",
    "        imgs_lr = Variable(validation_batch['LR'].type(self.Tensor))\n",
    "        imgs_hr = Variable(validation_batch['HR'].type(self.Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(self.Tensor(np.ones((imgs_lr.size(0), *self.netD.output_shape))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *self.netD.output_shape))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Validate Generators\n",
    "        # ------------------\n",
    "\n",
    "        # Generate a high resolution image from low resolution input\n",
    "        gen_hr = self.netG(imgs_lr)\n",
    "\n",
    "        # Measure pixel-wise loss against ground truth\n",
    "        loss_pixel = self.criterion_pixel(gen_hr, imgs_hr)\n",
    "\n",
    "        # if batches_done < warmup_batches:\n",
    "        #     # Warm-up (pixel-wise loss only)\n",
    "        #     loss_pixel.backward()\n",
    "        #     optimizer_G.step()\n",
    "        #     print(\n",
    "        #         \"[Epoch %d/%d] [Batch %d/%d] [G pixel: %f]\"\n",
    "        #         % (epoch, epochs, i, len(tra_dataloader), loss_pixel.item())\n",
    "        #     )\n",
    "        #     continue\n",
    "\n",
    "        # Extract validity predictions from discriminator\n",
    "        pred_real = self.netD(imgs_hr).detach()\n",
    "        pred_fake = self.netD(gen_hr)\n",
    "\n",
    "        # Adversarial loss (relativistic average GAN)\n",
    "        loss_GAN = self.criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), valid)\n",
    "\n",
    "        # Content loss\n",
    "        gen_features = self.netF(torch.repeat_interleave(gen_hr,3,1))\n",
    "        real_features = self.netF(torch.repeat_interleave(imgs_hr,3,1)).detach()\n",
    "        loss_content = self.criterion_content(gen_features, real_features)\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_G = loss_content + lambda_adv * loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        # ---------------------\n",
    "        #  Validate Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        pred_real = self.netD(imgs_hr)\n",
    "        pred_fake = self.netD(gen_hr.detach())\n",
    "\n",
    "        # Adversarial loss for real and fake images (relativistic average GAN)\n",
    "        loss_real = self.criterion_GAN(pred_real - pred_fake.mean(0, keepdim=True), valid)\n",
    "        loss_fake = self.criterion_GAN(pred_fake - pred_real.mean(0, keepdim=True), fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "        self.metric['val_loss_G'].append(loss_G.item())\n",
    "        self.metric['val_loss_content_G'].append(loss_content.item())\n",
    "        self.metric['val_loss_adversarial_G'].append(loss_GAN.item())\n",
    "        self.metric['val_loss_pixel_G'].append(loss_pixel.item())\n",
    "        self.metric['val_loss_D'].append(loss_D.item())\n",
    "        return gen_hr\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        training_loader = self.training_loader\n",
    "        validation_loader = self.validation_loader\n",
    "        sys.stdout.flush()\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch %d'%(epoch+1))\n",
    "            with tqdm(desc=('Training'), total=len(training_loader)) as pbar:\n",
    "                for i, (training_batch, validation_batch) in enumerate(itertools.zip_longest(training_loader,\n",
    "                                                                           validation_loader)):\n",
    "                    batches_done = epoch * len(tra_dataloader) + i\n",
    "                    gen_hr_train = self.training(training_batch)\n",
    "                    gen_hr_val = self.validate(training_batch)\n",
    "\n",
    "                    if batches_done % self.sample_interval == 0:\n",
    "                        img_grid = torch.cat((training_batch['LR'].to(device), training_batch['HR'].to(device), gen_hr_val, (torch.abs(training_batch['LR'].to(device)-gen_hr_val)*2)), -1)\n",
    "                        path = os.path.join(os.getcwd(), 'images', '%d.png' % batches_done)\n",
    "                        save_image(img_grid, path, nrow=1, normalize=False)\n",
    "\n",
    "\n",
    "                    it_metrics = {\n",
    "                        \"train_loss_G\": self.metric[\"train_loss_G\"][-1],\n",
    "                        \"train_loss_D\": self.metric[\"train_loss_D\"][-1],\n",
    "                    }\n",
    "                    pbar.set_postfix(**it_metrics)\n",
    "                    pbar.update()\n",
    "            torch.save(generator.state_dict(), \"saved_models/generator_%d.pth\" % epoch)\n",
    "            torch.save(discriminator.state_dict(), \"saved_models/discriminator_%d.pth\" %epoch)\n",
    "        sys.stdout.flush()\n",
    "        return self.metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1750 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 9.40 GiB already allocated; 21.69 MiB free; 9.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_1062696/4103765325.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m )\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_1062696/3676844790.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, epochs)\u001B[0m\n\u001B[1;32m    205\u001B[0m                                                                            validation_loader)):\n\u001B[1;32m    206\u001B[0m                     \u001B[0mbatches_done\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtra_dataloader\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 207\u001B[0;31m                     \u001B[0mgen_hr_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    208\u001B[0m                     \u001B[0mgen_hr_val\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalidate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_1062696/3676844790.py\u001B[0m in \u001B[0;36mtraining\u001B[0;34m(self, train_batch)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m         \u001B[0;31m# Content loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 94\u001B[0;31m         \u001B[0mgen_features\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnetF\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrepeat_interleave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgen_hr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     95\u001B[0m         \u001B[0mreal_features\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnetF\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrepeat_interleave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimgs_hr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     96\u001B[0m         \u001B[0mloss_content\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcriterion_content\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgen_features\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreal_features\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/beta/djboonstoppel/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/beta/djboonstoppel/Code/feature_extractor.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvgg19_54\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/mnt/beta/djboonstoppel/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/beta/djboonstoppel/miniconda/lib/python3.9/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/beta/djboonstoppel/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/beta/djboonstoppel/miniconda/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    444\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    445\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 446\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    447\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    448\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mConv3d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_ConvNd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/mnt/beta/djboonstoppel/miniconda/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    440\u001B[0m                             \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    441\u001B[0m                             _pair(0), self.dilation, self.groups)\n\u001B[0;32m--> 442\u001B[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001B[0m\u001B[1;32m    443\u001B[0m                         self.padding, self.dilation, self.groups)\n\u001B[1;32m    444\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 9.40 GiB already allocated; 21.69 MiB free; 9.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    feature_extractor=feature_extractor,\n",
    "    training_loader=tra_dataloader,\n",
    "    validation_loader=val_dataloader,\n",
    ")\n",
    "\n",
    "metrics = trainer.fit(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}